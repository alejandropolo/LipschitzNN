############## DEPENDENCIES ##############
import numpy as np
import ast
import itertools
import matplotlib.pyplot as plt
from scipy.spatial import Voronoi, voronoi_plot_2d
from shapely.geometry import Polygon, Point
from matplotlib.patches import Circle
import plotly.graph_objects as go
import torch
from neuralsens import partial_derivatives as ns
from neuralsens.partial_derivatives import calculate_second_partial_derivatives_mlp,calculate_first_partial_derivatives_mlp
###############################################################################################
########################################## FUNCTIONS ##########################################
###############################################################################################

############################### GENERATE HYPERCUBE VERTICES
def generate_hypercube_vertices(intervals):
    """
    Generate vertices for a hypercube (n-dimensional cube) defined by the given intervals.

    Parameters:
    - intervals (list of tuples): A list of tuples representing the intervals for each dimension.
      Each tuple should contain two values: the minimum and maximum values for that dimension.

    Returns:
    - vertices (list of tuples): A numpy array where each tuple represents a vertex of the hypercube.
      The vertices are generated by taking all possible combinations of values within the specified intervals (cartesian product).

    This function generates vertices for a hypercube in n-dimensional space, where n is determined by the number of intervals provided.

    Note: The number of vertices is equal to 2^n, where n is the number of dimensions, and each dimension's range is defined by the corresponding interval.
    """
    # Generate all possible combinations of values within the intervals
    vertices = list(itertools.product(*intervals))
    return np.array(vertices)

""" EXAMPLE OF USE
# Example usage:
n = 3  # Number of variables
intervals = [(0, 1), (0, 1), (0, 1)]  # Example intervals for each variable

vertices = generate_hypercube_vertices(intervals)
for vertex in vertices:
    print(vertex)
"""

############################### EXTRACTION OF FACES OF THE HYPERCUBE

def extract_hypercube_faces(vertices,limits):
    """
    Extract the faces of a hypercube defined by its vertices and limits.

    Parameters:
    - vertices (numpy.ndarray): An array containing the vertices of the hypercube.
    - limits (list of tuples): A list of tuples representing the intervals for each dimension.
      Each tuple should contain two values: the minimum and maximum values for that dimension.

    Returns:
    - faces (dict): A dictionary where keys are face labels, and values are numpy arrays representing the vertices of each face.

    This function extracts the faces of a hypercube defined by its vertices and limits. It uses the limits to identify and separate the vertices into different faces based on each dimension.

    Note: The input 'vertices' should be a numpy array, and 'limits' should be a list of tuples specifying the dimension intervals.
    EXAMPLE OF USE:
    intervals = [(0, 1), (0, 1), (0, 1)]
    faces = extract_hypercube_faces(vertices,intervals)
    faces
    """
    faces = dict()
    for i,limit in enumerate(limits):
        faces['x_{}{}'.format(i,limit[0])] = vertices[vertices[:,i]==limit[0]]
        faces['x_{}{}'.format(i,limit[1])] = vertices[vertices[:,i]==limit[1]]
    return faces


############################### EXTRACTION OF NORMAL VECTOR OF A HYPERPLANE

""" 
Justificación del método:
P1) Se substrae el centroide para generar vectores que deben formar parte del plano https://math.stackexchange.com/questions/3501135/fitting-a-plane-to-points-using-svd
P2) A partir de la matriz de vectores, se usa la descomposición en valores singulares (SVD) para obtener los vectores del kernel
P3) Como se trata de un hiperplano, solo hay un vector en el kernel que esta dado por la última columna de la matriz vh
https://math.stackexchange.com/questions/3359693/what-is-the-true-meaning-of-using-svd-in-finding-null-space
https://math.stackexchange.com/questions/1657447/null-space-for-mathcalna-given-svd-of-a
https://math.stackexchange.com/questions/2723294/how-to-determine-the-equation-of-the-hyperplane-that-contains-several-points
"""
def get_normal_vector(hyperplane_points, centroid_hypercube):
    """
    Compute the normal vector of a hyperplane defined by a set of points and ensure it points outward from a hypercube centroid.

    Parameters:
        hyperplane_points (array-like): An array of points defining the hyperplane.
        centroid_hypercube (array-like): The centroid of the hypercube.

    Returns:
        array-like: The normal vector of the hyperplane, adjusted to point outward from the hypercube centroid.

    This function calculates the normal vector of a hyperplane defined by a set of points in `hyperplane_points`. 
    It first computes the centroid of these points and then determines the normal vector using singular value decomposition (SVD). 
    The function also ensures that the normal vector points outward from the hypercube centroid by comparing it to a vector from 
    the centroid to a point on the hyperplane.

    If the normal vector points inward, it is inverted to ensure it points outward. The adjusted normal vector is then returned.
    """
    centroid = np.mean(hyperplane_points, axis=0)
    _, _, vh = np.linalg.svd(hyperplane_points - centroid, full_matrices=True)
    normal_vector = vh[-1]
    
    # Vector desde el centroide del hipercubo a un punto en la cara
    point_on_face = hyperplane_points[0]  # Elige un punto en la cara
    vector_to_face = point_on_face - centroid_hypercube
    
    # Producto escalar para verificar la orientación
    dot_product = np.dot(normal_vector, vector_to_face)

    # Si el producto escalar es negativo, invierte el vector normal
    if dot_product < 0:
        normal_vector = -normal_vector
    
    return normal_vector


############################### GENERATE SYMMETRIC POINTS

"""
La fórmula para symmetric_point se compone de dos partes:

np.dot(normal_vector, point_vector) calcula la proyección de point_vector sobre normal_vector. 
Esta proyección representa la distancia desde el punto original point hasta el hiperplano en la dirección del vector normal. 
Multiplicar esta distancia por 2 significa que estamos retrocediendo dos veces esa distancia desde el punto original point, 
cruzando el hiperplano y yendo una distancia igual al lado opuesto del hiperplano.

Luego, restamos esta distancia al punto original point para obtener el punto simétrico symmetric_point. 
Al restar la distancia en la dirección del vector normal, el nuevo punto se encuentra en el lado opuesto del hiperplano a una distancia igual. 
"""
def calculate_symmetric_points_faces(point, hyperrectangle_faces):
    ## Initialization of of the list of symmetric points
    symmetric_points = []
    ## Concatenate all the faces in a single array
    faces_array = np.concatenate(list(hyperrectangle_faces.values()))
    ## Drop the repeated points
    faces_array = np.unique(faces_array,axis=0)
    centroid_hypercube = np.mean(faces_array, axis=0)
    for face in hyperrectangle_faces.keys():
        hyperrectangle_vertices = hyperrectangle_faces[face]
        normal_vector = get_normal_vector(hyperrectangle_vertices,centroid_hypercube)
        ## Convertimos el normal vector a vector unitario
        normal_vector = normal_vector / np.linalg.norm(normal_vector)
        point_vector = point - hyperrectangle_vertices[0] #### ¿PORQUE FUNCIONA CUANDO USO POINT VECTOR Y NO CON POINT?
        ### Se calcula el valor simétrico
        ## The symmetric point is calculated (Given a vector n, the symmetric vector is given by: v-2*(v.n)*n)
        symmetric_point = point - 2 * np.dot(point_vector,normal_vector) * normal_vector
        symmetric_points.append(symmetric_point)
    return symmetric_points


############################### CHECK IF A POINT IS INSIDE A HYPERCUBE

def is_inside_hypercube(point,hypercube_vertices):
    """
    Check if a point is inside a hypercube defined by its vertices.

    This function determines whether a given point resides inside a hypercube.
    A hypercube is a multi-dimensional cube, and its vertices are used to define its boundaries.
    The function checks if every coordinate of the input point falls within the boundaries
    of the hypercube. In cases where a coordinate is equal to the minimum or maximum
    value of the hypercube, it is still considered inside the hypercube.

    Parameters:
    point (array-like): The coordinates of the point to be checked for inclusion in the hypercube.
    hypercube_vertices (array-like): An array containing the vertices of the hypercube, 
                                    where each row represents a vertex and each column a coordinate.

    Returns:
    bool: True if the point is inside the hypercube, False otherwise.

    Examples:
    >>> point = [0.5, 0.5, 0.5]
    >>> hypercube_vertices = np.array([[0, 0, 0], [1, 1, 1]])
    >>> is_inside_hypercube(point, hypercube_vertices)
    True

    >>> point = [1.2, 0.5, 0.5]
    >>> hypercube_vertices = np.array([[0, 0, 0], [1, 1, 1]])
    >>> is_inside_hypercube(point, hypercube_vertices)
    False
    """
    ## Check if the point is inside the hypercube
    ## Check if every coordinate is between the minimum and maximum values of the hypercube
    ## In case of equality, the point is considered inside the hypercube
    ## If any coordinate is outside the hypercube, the point is considered outside the hypercube
    return np.all(np.logical_and(np.min(hypercube_vertices, axis=0) <= point, point <= np.max(hypercube_vertices, axis=0)))

############################### PROYECTION TO THE HYPERCUBE

def proyection_hypercube(point, hypercube):
    """
    Given a point and a hypercube, returns the projection of the point onto the hypercube.

    Args:
        point (numpy.ndarray): The point to be projected.
        hypercube (numpy.ndarray): The hypercube defined by its vertices.

    Returns:
        numpy.ndarray: The projected point.

    Example of use:
    n = 3  # Number of variables
    intervals = [(0, 1), (0, 2), (0, 0.5)]  # Example intervals for each variable

    vertices = utilities_voronoi.generate_hypercube_vertices(intervals)
    point = np.array([1.1,2.5,0.2]) 
    proyection_hypercube(point,vertices) -> (1,2.0,0.2)
    """
    # Check if the point is inside the hypercube
    if is_inside_hypercube(point, hypercube):
        # If the point is inside the hypercube, return it as is
        return point
    else:
        # If the point is outside the hypercube, project it onto the hypercube
        # The projection consists of reducing the coordinates that are outside the hypercube
        # to the minimum or maximum values of the hypercube

        # Extract the minimum and maximum coordinates of the hypercube
        min_coords = hypercube.min(axis=0)
        max_coords = hypercube.max(axis=0)

        # Iterate over each coordinate of the point
        projected_point = []
        for i in range(len(point)):
            # Check if the coordinate is within the hypercube
            if point[i] < min_coords[i]:
                projected_point.append(min_coords[i])
            elif point[i] > max_coords[i]:
                projected_point.append(max_coords[i])
            else:
                projected_point.append(point[i])

        # Return the projected point as a numpy array
        return np.array(projected_point)

############################### ADD SYMMETRIC POINTS

def add_symmetric_points(vor,vertices,intervals):

    ## Extract faces of the hypercube
    faces = extract_hypercube_faces(vertices,intervals)
    ## Lista para almacenar los puntos simétricos
    symmetric_points_list = []
    ## Iteramos en las regiones del voronoi
    for i, r in enumerate(vor.point_region):
        ## Extraemos la región para el punto i-ésimo
        region = vor.regions[r]
        
        ## Si la región contiene un -1 (region infinita) se añaden puntos simétricos
        if -1 in region:
            symmetric_points = calculate_symmetric_points_faces(vor.points[i], faces)
            symmetric_points_list.append(symmetric_points)
        ### Además comprueba si todos los vértices de la región están dentro del hipercubo
        ### Si no están dentro del hipercubo, se añaden puntos simétricos
        else:
            region_vertices = vor.vertices[region]
            if not is_inside_hypercube(region_vertices,vertices): ########################## CHEQUEAR QUE FUNCIONA BIEN CUANDO EN VEZ DE UN PUNTO SE PASAN MULTIPLES PUNTOS
                symmetric_points = calculate_symmetric_points_faces(vor.points[i], faces)
                symmetric_points_list.append(symmetric_points)

    ## Combine original and symmetric points
    all_points = np.vstack((vor.points, np.array(symmetric_points_list).reshape(-1, vertices.shape[1]))) ### GENERALIZAR PARA N DMIENSIONES
    return all_points, np.array(symmetric_points_list).reshape(-1, vertices.shape[1])

############################### CHECK IF THE SPACE IS FILLED

def check_space_filled(vor,dict_radios,vertices): 
    ################################ AÑADIR LA MISMA MODIFICACION QUE EN EL CASO VECTORIZED
    ## Extract radius value
    radius = np.array(list(dict_radios.values()))[:,0]
    ## Initialize distances dictionary
    distances = dict()
    # Find furthest vertices for each point
    for i,point in enumerate(vor.points):
        ## Extract the region index for the point i
        region_idx = vor.point_region[i]
        ## Check if the point is inside the boundary polygon
        if is_inside_hypercube(point,vertices): ##### REVISAR QUE ESTA CONDICIÓN SEA CORRECTA
            ## Extract the vertices of the region
            region_vertices = vor.vertices[vor.regions[region_idx]]
            ## Extract the furthest vertex
            furthest_vertex = region_vertices[np.argmax(np.linalg.norm(region_vertices - point, axis=1))]
            ## Compute distance to furthest vertex
            distance = np.linalg.norm(point-furthest_vertex)
            distances['{}'.format(point)] = distance
    ## Check if the space is filled
    if np.all(np.array(np.array(list(distances.values())))<=radius):
        return True ,distances
    else:
        return False ,distances


def check_space_filled_vectorized(vor, dict_radios, vertices):
    distances = []
    ## Extract radius value
    radius = np.array(list(dict_radios.values()))[:,0]
    
    ## Initialize the counter
    j = 0
    for i, point in enumerate(vor.points):
        region_idx = vor.point_region[i]
        if is_inside_hypercube(point, vertices):
            ## Check that the distance is goint to be calculated for the correct point
            point_radio = np.array(ast.literal_eval(list(dict_radios.keys())[j]), dtype=np.float32)
            if not np.allclose(point, point_radio, atol=1e-8):
                print('Point from voronoi set: ', point)
                print('Point from dict_radios: ', point_radio)
                raise ValueError('The points are not the same, and therefore the radio is not the correct one')
            region_vertices = vor.vertices[vor.regions[region_idx]]
            furthest_vertex = region_vertices[np.argmax(np.linalg.norm(region_vertices - point, axis=1))]
            distance = np.linalg.norm(point - furthest_vertex)
            distances.append(distance)
            j += 1

    distances_array = np.array(distances)

    if np.all(distances_array <= radius):
        return True, distances_array
    else:
        return False, distances_array


############################### CHECK IF A POINT IS INSIDE A HYPERCUBE
    
def points_inside_hypercube(points, vertices):
    """
    Returns the points and their indices that are inside the hypercube defined by the given vertices.

    Args:
        points (numpy.ndarray): An array of points.
        vertices (numpy.ndarray): An array of vertices defining the hypercube.

    Returns:
        Tuple[numpy.ndarray, numpy.ndarray]: A tuple containing the points and their indices that are inside the hypercube.
    """
    points_inside = np.array([point for i, point in enumerate(points) if is_inside_hypercube(point, vertices)])
    indices_inside = np.array([i for i, point in enumerate(points) if is_inside_hypercube(point, vertices)])
    return points_inside, indices_inside

############################### GET HESSIAN BOUND

def hessian_bound(W,actfunc,partial_monotonic_variable,n_variables):
    """_summary_
    La idea es generar un algoritmo recursivo de modo que si la red neuronal tiene k capas, entonces se comience calculando el hessiano acotado
    H_0_1, luego el hessiano acotado H_0_2, y así sucesivamente hasta llegar a H_0_k.
    Args:
        W (_type_): list of numpy arrays with the weights (W[i][1:,:]) and the biases (W[i][0,:]) of the network
    """
    ## In case there are multiple partial monotonic variables
    hessian_bounds = []
    for var in partial_monotonic_variable: 
        hessian_boud =[]
        ## First compute H_0^1
        ## Remember that ||H_0_1|| <= max|a_k_1|*||W_1_1||*||W_1|| 
        # where W_1_1 is the first row of the first layer of weights and a_k_1 is the maximum possible value of the second derivative of the activation function of the first layer
        
        ## Generate the vector (0,...,1(i),...,0)
        input_vector = [0] * n_variables
        input_vector[var] = 1

        if actfunc[1]=='sigmoid':
            a_1 = 0.25
        else:
            a_1 = 1
        ## W_1j^1 * W_1
        weights_multiplication = np.linalg.norm(input_vector @ W[1][1:, :], ord=2) * np.linalg.norm(W[1][1:, :], ord=2)
        H_0_1 = a_1*weights_multiplication
        hessian_boud.append(H_0_1)
        
        ## In general, H_0_k can be bounded following the next equation
        # H_0_k <= max|a_k|*||W_1_1||*||W_1||*(||W_2||^2)*...*(||W_{k+1}||^2) + H_0_{k-1}*||W_k||
        for k in range(2,len(actfunc)):
            if actfunc[k]=='sigmoid':
                a_k = 0.25
            elif actfunc[k]=='identity': 
                a_k = 0
            else:
                a_k = 1
            weights_multiplication = weights_multiplication * np.linalg.norm(W[k][1:, :], ord=2)**2
            H_0_k = a_k*weights_multiplication + hessian_boud[-1]*np.linalg.norm(W[k][1:, :], ord=2)
            hessian_boud.append(H_0_k)
        ## Return the last element of the list
        hessian_bounds.append(hessian_boud[-1])
    return max(hessian_bounds)

def get_weights_and_biases(model):
    """
    Retrieves the weights and biases from a PyTorch model.

    This function extracts the weights and biases from the layers of a PyTorch model.
    The weights and biases are extracted from the model's state dictionary and returned in two separate lists.

    Parameters:
    model (torch.nn.Module): The PyTorch model from which to extract weights and biases.

    Returns:
    tuple: A tuple containing two lists. The first list contains the weights from each layer of the model,
           and the second list contains the biases from each layer of the model.

    Example:
    >>> model = torch.nn.Linear(2, 3)
    >>> weights, biases = get_weights_and_biases(model)
    >>> print(weights)
    [tensor([[ 0.3643, -0.3121],
             [-0.1371,  0.3319],
             [-0.6657,  0.4241]])]
    >>> print(biases)
    [tensor([0.0925, 0.2071, 0.2133])]
    """
    parameters = dict(model.state_dict())
    weights = []
    biases = []
    for key, value in parameters.items():
        if 'weight' in key:
            weights.append(value.T)
        elif 'bias' in key:
            biases.append(value)
    return weights, biases

############################### GET LIPSCHITZ RADIUS

def get_lipschitz_radius(inputs,model,global_lipschitz_constant,monotone_relation,variable_index,n_variables):
    """
    Calculates the radius where the monotonicity is enforced for a given set of inputs and a neural network model.

    Parameters:
    inputs (list): List of input tensors.
    model (torch.nn.Module): Neural network model.
    global_lipschitz_constant (torch.Tensor): Global Lipschitz constant.
    monotone_relation (int): Monotone relation indicator. 1 for positive monotone relation, -1 for negative monotone relation.
    variable_index (int): Index of the variable to consider for the monotone relation and therefore for the Lipschitz radius calculation.
    n_variables (int): Number of variables in the input tensors.

    Returns:
    radius_tot (list): List of Lipschitz radii for each input.
    dict_radios (dict): Dictionary mapping input coordinates to their corresponding Lipschitz radii and relation indicators.
    x_reentrenamiento (torch.Tensor): Tensor containing the inputs that do not satisfy the monotone relation.

    Note:
    - The Lipschitz radius is calculated as the ratio between the derivative and the global Lipschitz constant.
    - The monotone relation determines whether the derivative should be positive or negative for the given variable.
    - The function currently only supports the same monotone relation for every variable
    """
    #################################################### AMPLIAR EL CÓDIGO PARA QUE PERMITA MULTIPLES MONOTONE RELATIONS
    """Idea para que admita relaciones monótonas crecientes y decrecientes.
    1) Se calcula el radio para cada variable
    2) Si se cumple la relación monótona (ya sea creciente o decreciente) para todas las variables, se toma el mínimo de los radios (Buscamos el espacio  en el que se cumplen todas las condiciones)
    3) Si no se cumple la relación monótona para alguna variable, se toma el máximo de los radios (Buscamos el espacio máximo en el que alguna de las condiciones falla)
    """
    ## Comenzamos definiendo las variables que luego iremos rellenando
    radius_tot = []
    dict_radios = {}
    x_reentrenamiento = torch.tensor([]).reshape(-1,n_variables)

    ## Para cada input, calculamos el radio como el cociente entre la derivada y la constante de lipschitz

    for x in inputs:
        input = x.clone().detach().requires_grad_(True)
        # Realiza una pasada hacia adelante para calcular la salida del modelo en 'x'
        output = model(input)
        # Calcula la derivada
        output.backward()
        # La derivada con respecto a 'x' ahora se encuentra en x.grad
        derivative = input.grad
        
        ## Se extrae la componente de la derivada que nos interesa
        derivative = derivative[variable_index]
        ## Compruebo si la derivada cuadra con la monotone relation (if monotone_relation 1, derivative must be positive) else (derivative must be negative))
        if monotone_relation == 1:
            ## Si alguno de los valores de las derivadas de las variables analizadas no cumple la relacion se añade al conjunto de reentrenamiento
            ## Chequeamos ahora si la derivada es 0 alguna de las variables consideradas
            if (derivative == 0).any():
                ## Añadimos el punto al conjunto de reentrenamiento
                x_reentrenamiento = torch.cat((x_reentrenamiento,input.reshape(-1,n_variables)),dim=0)
                ## El radio es 0
                r = 0
                radius_tot.append(r)
                #dict_radios['({},{})'.format(x.detach().numpy()[0],x.detach().numpy()[1])] = [r,0]
                dict_radios[repr(list(input.detach().numpy()))] = [r,0]
            ## Se comprueba si la derivada es mayor que 0 en todas las variables consideradas o en alguna de ellas es menor que 0
            ## Para ello se usa la función torch.relu que devuelve 0 si el valor es negativo y el valor si es positivo
            elif torch.sum(torch.relu(-derivative))>0:
                ## Añadimos el punto al conjunto de reentrenamiento
                x_reentrenamiento = torch.cat((x_reentrenamiento,input.reshape(-1,n_variables)),dim=0)
                ## Calculamos el radio como el máximos de las componentes negativas de la derivada (en valor absoluto -> Por eso tomamos la -derivative)
                ## Extraigo solo la parte de la derivada que no cumple la relacion
                derivative_neg = torch.relu(-derivative)
                ## Extraigo el maximo de la derivada que no cumple la relacion (Si no se cumple para una variable la relación da igual que para otra si que se cumpla)
                max_der = torch.max(derivative_neg).item()
                ## Divido entre la constante de lipschitz para obtener el radio 
                r = max_der/global_lipschitz_constant.item()
                radius_tot.append(r)
                #dict_radios['({},{})'.format(x.detach().numpy()[0],x.detach().numpy()[1])] = [r,-1]
                dict_radios[repr(list(input.detach().numpy()))] = [r,-1]
            ## En caso de que sea positivo para todas las variables
            else:
                ## Extraigo el mínimo de la derivada que cumple la relación
                # Tomo el mínimo porque voy a construir bolas cerradas en las que se cumple la relación y si se cumple para un r, en particular se cumple para todo r'<r 
                r = torch.min(derivative).item()/global_lipschitz_constant.item()
                radius_tot.append(r)
                #dict_radios['({},{})'.format(x.detach().numpy()[0],x.detach().numpy()[1])] = [r,1]
                dict_radios[repr(list(input.detach().numpy()))] = [r,1]
        elif monotone_relation == -1:
            ## Si alguno de los valores de las derivadas de las variables analizadas no cumple la relacion se añade al conjunto de reentrenamiento
            ## Chequeamos ahora si la derivada es 0 alguna de las variables consideradas
            if (derivative == 0).any():
                ## Añadimos el punto al conjunto de reentrenamiento
                x_reentrenamiento = torch.cat((x_reentrenamiento,input.reshape(-1,n_variables)),dim=0)
                ## El radio es 0
                r = 0
                radius_tot.append(r)
                #dict_radios['({},{})'.format(x.detach().numpy()[0],x.detach().numpy()[1])] = [r,0]
                dict_radios[repr(list(input.detach().numpy()))] = [r,0]
            ## Se comprueba si la derivada es menor que 0 en todas las variables consideradas o en alguna de ellas es mayor que 0
            elif torch.sum(torch.relu(derivative))>0:
                ## Añadimos el punto al conjunto de reentrenamiento
                x_reentrenamiento = torch.cat((x_reentrenamiento,input.reshape(-1,n_variables)),dim=0)
                ## Calculamos el radio como el máximo de las componentes positivas de la derivada (el mayor espacio en el que no se cumple alguna condicion)
                ## Extraigo solo la parte de la derivada que no cumple la relacion
                derivative_pos = torch.relu(derivative)
                ## Extraigo el maximo de la derivada que no cumple la relacion
                max_der = torch.max(derivative_pos).item()
                ## Divido entre la constante de lipschitz para obtener el radio 
                r = max_der/global_lipschitz_constant.item()
                radius_tot.append(r)
                #dict_radios['({},{})'.format(x.detach().numpy()[0],x.detach().numpy()[1])] = [r,1]
                dict_radios[repr(list(input.detach().numpy()))] = [r,1]
            
            ## En caso de que sea negativo para todas las variables
            else:
                ## Extraigo el mínimo de los valores para los que se cumplen la relación
                # Tomo el máximo porque voy a construir bolas cerradas en las que se cumple la relación y si se cumple para un r, en particular se cumple para todo r'<r 
                r = torch.min(-derivative).item()/global_lipschitz_constant.item()
                radius_tot.append(r)
                #dict_radios['({},{})'.format(x.detach().numpy()[0],x.detach().numpy()[1])] = [r,-1]
                dict_radios[repr(list(input.detach().numpy()))] = [r,-1]
        else:
            print('Por ahora el código solo esta preparado para relaciones monótonas crecientes')
    return radius_tot,dict_radios,x_reentrenamiento

def get_lipschitz_radius_neuralsens(inputs,outputs,weights,biases,actfunc,global_lipschitz_constant,monotone_relation,variable_index,n_variables, epsilon):
    radius_tot = []
    dict_radios = {}
    x_reentrenamiento = torch.tensor([]).reshape(-1,n_variables)

    _, _, _, _, D_accum, _, _ = calculate_first_partial_derivatives_mlp(weights, biases, actfunc, inputs, outputs,sens_end_layer=len(actfunc))
    derivatives = D_accum[-1]
    no_points = True
    for i,der in enumerate(derivatives):
        x = inputs[i]
        derivative = torch.tensor(der.flatten()).float()[variable_index]
        if (derivative == 0).any():
            x_reentrenamiento = torch.cat((x_reentrenamiento,x.reshape(-1,n_variables)),dim=0)
            r = 0
            radius_tot.append(r)
            dict_radios[repr(list(x.detach().numpy()))] = [r,0]
        elif torch.sum(torch.relu(-monotone_relation*derivative))>0:
            no_points = False
            x_reentrenamiento = torch.cat((x_reentrenamiento,x.reshape(-1,n_variables)),dim=0)
            derivative_neg = torch.relu(-monotone_relation*derivative)
            max_der = torch.max(derivative_neg).item()
            r = max_der/global_lipschitz_constant.item()
            radius_tot.append(r)
            dict_radios[repr(list(x.detach().numpy()))] = [r,-1*monotone_relation]
        else:
            r = torch.min(monotone_relation*derivative).item()/global_lipschitz_constant.item()
            radius_tot.append(r)
            dict_radios[repr(list(x.detach().numpy()))] = [r,monotone_relation]

    if no_points:
        for i,der in enumerate(derivatives):
            x = inputs[i]
            derivative = torch.tensor(der.flatten()).float()[variable_index]
            if not (derivative == 0).any() and not torch.sum(torch.relu(-monotone_relation*derivative))>0:
                if torch.min(monotone_relation*derivative).item() < epsilon:
                    x_reentrenamiento = torch.cat((x_reentrenamiento,x.reshape(-1,n_variables)),dim=0)

    return radius_tot,dict_radios,x_reentrenamiento

############################### ADD POINTS TO THE VORONOI DIAGRAM
def add_new_point_vectorized(finite_vor, vertices, distances, dict_radios, probability=0.1):
    np.random.seed(seed=0)
    min_covered_points = float('inf')
    max_radio = float('-inf')
    min_radio = float('inf')
    selected_vertex_max = None
    selected_vertex_min = None

    ## Extract radios values
    radios = np.array(list(dict_radios.values()))[:,0] ## The second column is the relation indicator

    points = finite_vor.points
    point_inside, indices_inside = points_inside_hypercube(points, vertices)

    assert len(indices_inside) == len(distances) == len(radios), "The number of points inside the voronoi set is not the same as the distances and the radios"

    ## Loop over the distances and the radios
    for i, distance in enumerate(distances):
        ## Check if the distance is greater than the radio (If not the Voronoi cell is already filled)
        if distance > radios[i]:
            ## Extract the point and the region index
            point = finite_vor.points[indices_inside[i]]
            ## Check that the radio matches the point
            point_radio = np.array(ast.literal_eval(list(dict_radios.keys())[i]),dtype=np.float32)
            assert np.allclose(point, point_radio, atol=1e-8), "The points are not the same, and therefore the radio is not the correct one"
            
            ## Extract the region index and vertices for the point i
            region_idx = finite_vor.point_region[indices_inside[i]]
            region_vertices = finite_vor.vertices[finite_vor.regions[region_idx]]
            
            ## Extract the furthest vertex
            furthest_vertex = region_vertices[np.argmax(np.linalg.norm(region_vertices - point, axis=1))]

            # Precompute distances between point_inside and furthest_vertex
            distances_to_furthest = np.linalg.norm(point_inside - furthest_vertex, axis=1)

            # Count covered points using vectorized operations
            covered_points = np.sum(distances_to_furthest < radios)

            #if covered_points < min_covered_points:
            ## If the number of covered points is less than the minimum covered points or the number of covered points is the same but the radio is greater
            if covered_points < min_covered_points or (covered_points == min_covered_points and radios[i] < min_radio):
                    min_covered_points = covered_points
                    min_radio = radios[i]
                    selected_vertex_min = furthest_vertex
            elif covered_points < min_covered_points or (covered_points == min_covered_points and radios[i] > max_radio):
                    min_covered_points = covered_points
                    max_radio = radios[i]
                    selected_vertex_max = furthest_vertex

    if np.random.rand() < probability:
        selected_vertex = selected_vertex_min
    else:
        selected_vertex = selected_vertex_max
    return selected_vertex

def add_points_to_voronoi(original_vor, original_points, finite_vor, dict_radios, vertices, distances, 
                          model,actfunc, global_lipschitz_constant, intervals,monotone_relations,variable_index,
                          n_variables,mode='neuralsens',plot_voronoi=False, epsilon=1e-5, max_iterations=10):
    """
    Add points to a Voronoi diagram using the furthest vertex for each point.

    Args:
        original_vor (scipy.spatial.Voronoi): The original Voronoi diagram.
        original_points (numpy.ndarray): The original points in the Voronoi diagram.
        finite_vor (scipy.spatial.Voronoi): The finite Voronoi diagram (with the added symmetric points)
        vertices (numpy.ndarray): The vertices defining the hypercube.
        distances (dict): The distances for each point in the Voronoi diagram.
        model (torch.nn.Module): The trained model.
        global_lipschitz_constant (float): The global Lipschitz constant.
        x_lim (tuple): The x-axis limits of the hypercube.
        y_lim (tuple): The y-axis limits of the hypercube.
        monotone_relations (list): The monotone relations for each variable.
        variable_index (list): The indices of the variables to compute the local Lipschitz constant.
        n_variables (int): The number of variables.
        plot_voronoi (bool, optional): Whether to plot each Voronoi diagram. Defaults to False.
        epsilon (float, optional): The extension of the hypercube. Defaults to 1e-5. It is needed to compute symmetric points on the boundary.
        max_iterations (int, optional): The maximum number of iterations. Defaults to 10.

    Returns:
        numpy.ndarray: The updated original points in the Voronoi diagram.
    """
    ##################################################    MODIFICAR PARA QUE INCLUYA DICT RADIOS Y NO RADIOS
    ## Define the coordinates of the square's vertices
    #square_vertices = np.array([[x_lim[0], y_lim[0]], [x_lim[0], y_lim[1]], [x_lim[1], y_lim[1]], [x_lim[1], y_lim[0]], [x_lim[0], y_lim[0]]])

    ## Generate vertices for a hypercube (n-dimensional cube) defined by the given interval with the given extension
    #intervals_extended = [(x_lim[0] - epsilon, x_lim[1] + epsilon), (y_lim[0] - epsilon, y_lim[1] + epsilon)]
    intervals_extended = [(x - epsilon, y + epsilon) for x, y in intervals]
    vertices_extended = generate_hypercube_vertices(intervals_extended)

    ## Boolean warning to print if there are points not following the monotone relation
    warning = False
    
    if mode == 'neuralsens':
        print('Using NeuralSens')
        weights, biases = get_weights_and_biases(model)
    elif mode == 'autograd':
        print('Using autograd')
    else:
        raise ValueError('The mode must be either autograd or neuralsens')

    for i in range(max_iterations):
        ## Add new point
        #selected_vertex = add_new_point(finite_vor, vertices, distances, dict_radios)
        selected_vertex = add_new_point_vectorized(finite_vor, vertices, distances, dict_radios)
        ## Project the new point to the hypercube (because of the extension it may be outside the hypercube)
        selected_vertex = proyection_hypercube(selected_vertex, vertices)

        ## Checks if selected vertex is already in the original points
        ## In that case the loop has to stop because the dictionary cannot have two arrays with the same key
        if np.any(np.all(np.isclose(original_points, selected_vertex,rtol=1e-07), axis=1)):
            print('The selected vertex is already in the original points and the vertex is {}'.format(selected_vertex))
            break
        else:
            ## Add the new point to the original points
            original_points = np.vstack((original_points, selected_vertex))
        ## Add the new point to the inputs
        inputs = torch.tensor(original_points, dtype=torch.float)
        
        ## Add the new point to the Voronoi diagram
        original_vor.add_points(selected_vertex.reshape(1, -1))
        ## Compute the new finite Voronoi diagram with the new point
        ##MODIFICACIÓN PARA NO TENER QUE RECALCULAR EL VORONOI
        all_points, _ = add_symmetric_points(original_vor, vertices_extended, intervals_extended)
        finite_vor = Voronoi(all_points, incremental=True)
        
        """ ## Para intentar evitar recalcular
        all_points, symmetric_points = add_symmetric_points(original_vor, vertices_extended, intervals_extended)
        ### Check if symmetric points is not an empty array:
        print(symmetric_points)
        if symmetric_points.shape[0]!=0:        
            finite_vor.add_points(np.array(symmetric_points))"""

        ## Compute the new radios for each point
        if mode=='autograd':
            radius_tot, dict_radios, x_reentrenamiento = get_lipschitz_radius(inputs=inputs, model=model, global_lipschitz_constant=global_lipschitz_constant, 
                                                                            monotone_relation=monotone_relations, variable_index=variable_index, n_variables=n_variables)
        elif mode=='neuralsens':
            radius_tot, dict_radios, x_reentrenamiento = get_lipschitz_radius_neuralsens(inputs=inputs, outputs=[], weights=weights, biases=biases, actfunc=actfunc, 
                                                                                        global_lipschitz_constant=global_lipschitz_constant, 
                                                                                        monotone_relation=monotone_relations, variable_index=variable_index, 
                                                                                        n_variables=n_variables)

        derivative_sign = [v[1] for _, v in dict_radios.items()]
        ## Plot Voronoi diagram
        if plot_voronoi and len(intervals) == 2:
            plot_finite_voronoi_2D(vor=finite_vor, all_points=all_points, original_points=original_points, radios=radius_tot, boundary=vertices, derivative_sign=derivative_sign, plot_symmetric_points=False)
        ## Check if the space is filled
        #space_filled, distances = check_space_filled(finite_vor, radius_tot, vertices)
        space_filled, distances = check_space_filled_vectorized(finite_vor, dict_radios, vertices)
        ## Check if the space is filled and if x_reentrenamiento is empty
        if space_filled and x_reentrenamiento.shape[0]==0:
            print('The space is filled: {} after {} iterations '.format(space_filled,i+1))
            break
        elif x_reentrenamiento.shape[0]!=0 and not warning:
            print('The retraining set is not empty and therefore the space cannot be filled: {} points'.format(x_reentrenamiento.shape[0]))
            warning = True

    if len(intervals) == 2:
        plot_finite_voronoi_2D(vor=finite_vor, all_points=all_points, original_points=original_points, radios=radius_tot, boundary=vertices, derivative_sign=derivative_sign, plot_symmetric_points=False)

    return x_reentrenamiento


def plot_finite_voronoi_2D(vor,all_points,original_points,radios,boundary,derivative_sign,plot_symmetric_points=False):
    # Plot Voronoi diagram with dashed lines for finite regions
    fig, ax = plt.subplots(figsize=(8, 8))
    ## Comenzamos definiendo el boundary polygon
    ## Check if fist and last element from boundary is the same if not add it
    if not np.array_equal(boundary[0],boundary[-1]):
       boundary = np.vstack((boundary,boundary[0]))
    boundary_polygon = Polygon(boundary)

    # Plot Voronoi regions
    if plot_symmetric_points:
        for region in vor.regions:
            if -1 not in region and len(region) > 0:
                polygon = [vor.vertices[i] for i in region]
                ##plt.fill(*zip(*polygon), alpha=0.5) ##Fill the polygon
                polygon_draw = Polygon(polygon)
                x,y = polygon_draw.exterior.xy
                plt.plot(x,y,color='k')
    else:
        for i,region in enumerate(vor.regions):
            ## Check if the point is inside the boundary polygon
            if -1 not in region and len(region) > 0:
                point_index = np.where(vor.point_region == i)[0][0]
                point = vor.points[point_index]
                ##Check if the point is inside the boundary polygon
                #if Polygon(boundary).contains(Point(point)):
                if is_inside_hypercube(point,boundary):
                    polygon = [vor.vertices[i] for i in region]
                    ##plt.fill(*zip(*polygon), alpha=0.5) ##Fill the polygon
                    polygon_draw = Polygon(polygon)
                    x,y = polygon_draw.exterior.xy
                    plt.plot(x,y,color='k')

    # Find furthest vertices and draw dashed lines for original points
    for i in range(len(all_points)):
        region_idx = vor.point_region[i]
        region = vor.regions[region_idx]
        ## Check if the region is finite and if the furthest vertex is inside the boundary polygon
        #if boundary_polygon.contains(Point(all_points[i])):
        if is_inside_hypercube(all_points[i],boundary):
            region_vertices = vor.vertices[vor.regions[region_idx]]
            furthest_vertex = region_vertices[np.argmax(np.linalg.norm(region_vertices - all_points[i], axis=1))]
            # Draw dashed line
            plt.plot([all_points[i][0], furthest_vertex[0]], [all_points[i][1], furthest_vertex[1]], linestyle='--',color='purple')


    x,y = boundary_polygon.exterior.xy
    #plt.plot(x, y, 'b-', label='Square')
    # Plot original points
    ## Dibujamos ahor alos puntos simulados
    if plot_symmetric_points:
        plt.plot(all_points[:, 0], all_points[:, 1], 'go')
    plt.plot(original_points[:, 0], original_points[:, 1], 'ro', markersize=2)

    ## Añadimos al dibujo los radios aleatorios
    for i in range(len(original_points)):
        if derivative_sign[i]==1:
            plt.gca().add_patch(Circle(original_points[i], radios[i], color='b', alpha=0.25))
        else:
            plt.gca().add_patch(Circle(original_points[i], radios[i], color='r', alpha=0.25))

    if plot_symmetric_points:
        plt.title('Voronoi Diagram with Symmetric Points')
        plt.xlim(vor.min_bound[0] - 0.2, vor.max_bound[0] + 0.2)
        plt.ylim(vor.min_bound[1] - 0.2, vor.max_bound[1] + 0.2)
    else:
        plt.title('Voronoi Diagram')
        ## Define the xlim and ylim using the boundary polygon
        plt.xlim(min(x)-0.2, max(x)+0.2)
        plt.ylim(min(y)-0.2, max(y)+0.2)

    plt.gca().set_aspect('equal', adjustable='box')
    plt.show()


###### PLOT VORONOI IN 3D
def ms(x, y, z, radius, resolution=20):
    """Return the coordinates for plotting a sphere centered at (x,y,z)"""
    u, v = np.mgrid[0:2*np.pi:resolution*2j, 0:np.pi:resolution*1j]
    X = radius * np.cos(u)*np.sin(v) + x
    Y = radius * np.sin(u)*np.sin(v) + y
    Z = radius * np.cos(v) + z
    return (X, Y, Z)    

def plot_finite_voronoi_3D(vor,all_points,original_points,radios,vertices,plot_symmetric_points=False):

    # Crea una lista para almacenar los polígonos de Voronoi
    polygons = []
    radii = np.random.rand(original_points.shape[0])
    # Para cada celda de Voronoi, crea un objeto Polygon3D
    """for region in vor.regions:
        if not -1 in region and len(region) > 0:
            region_vertices = vor.vertices[region]
            #if is_inside_hypercube(region_vertices,vertices):
            region_vertices = [vor.vertices[i] for i in region]
            ## Check that every vertex is inside 
            polygons.append(go.Mesh3d(x=[v[0] for v in region_vertices],
                                        y=[v[1] for v in region_vertices],
                                        z=[v[2] for v in region_vertices],
                                        opacity=0.1, alphahull=0, colorscale='Viridis'))"""
    for region_index, region in enumerate(vor.regions):
        if not -1 in region and len(region) > 0:
            region_vertices = vor.vertices[region]
            voronoi_point = vor.points[vor.point_region == region_index][0]
            if is_inside_hypercube(voronoi_point, vertices):
                region_vertices = [vor.vertices[i] for i in region]
                polygons.append(go.Mesh3d(x=[v[0] for v in region_vertices],
                                        y=[v[1] for v in region_vertices],
                                        z=[v[2] for v in region_vertices],
                                        opacity=0.1, alphahull=0, colorscale='Viridis'))

    # Crea una gráfica 3D de dispersión para los puntos de entrada
    if plot_symmetric_points:
        scatter = go.Scatter3d(x=all_points[:, 0], y=all_points[:, 1], z=all_points[:, 2], mode='markers', marker=dict(size=5))
    scatter_original = go.Scatter3d(x=original_points[:, 0], y=original_points[:, 1], z=original_points[:, 2], mode='markers', marker=dict(size=5))

    data_sph =[]
    ## PLOT SPHERES
    for i,point in enumerate(original_points):
        x_pns_surface, y_pns_surface, z_pns_suraface = ms(point[0], point[1], point[2], radios[i])
        data_sph.append(go.Surface(x=x_pns_surface, y=y_pns_surface, z=z_pns_suraface, colorscale='reds', opacity=0.2, showscale=False))
    # Crea la figura 3D
    if plot_symmetric_points:
        fig = go.Figure(data=[scatter] +[scatter_original]+ polygons)
    else:
        fig = go.Figure(data=[scatter_original]+ polygons + data_sph)



    # Limita los valores de x, y y z al máximo y mínimo de los puntos originales
    x_min, x_max = np.min(original_points[:, 0]) - 1, np.max(original_points[:, 0]) + 1
    y_min, y_max = np.min(original_points[:, 1]) - 1, np.max(original_points[:, 1]) + 1
    z_min, z_max = np.min(original_points[:, 2]) - 1, np.max(original_points[:, 2]) + 1

    # Configura las opciones de diseño
    fig.update_layout(scene=dict(xaxis=dict(range=[x_min, x_max]), 
                                yaxis=dict(range=[y_min, y_max]), 
                                zaxis=dict(range=[z_min, z_max]), 
                                aspectmode="cube"),
                    height=1000, width=800)

    # Muestra la gráfica
    fig.show()
